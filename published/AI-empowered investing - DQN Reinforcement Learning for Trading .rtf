{\rtf1\ansi\ansicpg1252\cocoartf2818
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Bold;\f1\froman\fcharset0 Times-Roman;\f2\fmodern\fcharset0 Courier;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red109\green109\blue109;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c50196\c50196\c50196;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid6}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}}
\paperw11900\paperh16840\margl1440\margr1440\vieww15380\viewh15440\viewkind0
\deftab720
\pard\pardeftab720\sa321\partightenfactor0

\f0\b\fs48 \cf0 \expnd0\expndtw0\kerning0
AI-empowered investing - DQN Reinforcement Learning for Trading 
\f1\b0\fs24 \
\pard\pardeftab720\sa240\partightenfactor0
\cf0 Advanced machine learning techniques including deep learning, reinforcement learning, and ensemble methods empower platforms that provide retail investors with access to cutting-edge AI capabilities for investment analysis and portfolio management.\
\pard\pardeftab720\sa240\partightenfactor0

\f0\b \cf0 DQN
\f1\b0  is a 
\f0\b reinforcement learning (RL)
\f1\b0  algorithm that combines 
\f0\b Q-Learning
\f1\b0  with 
\f0\b deep neural networks
\f1\b0 . It\'92s especially useful when the state or environment is complex like in trading.\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 Platforms like 
\f0\b Moolah
\f1\b0 , which is a pioneer for LLM-powered funds and AI-trading, incorporate DQN-RL algorithms into its investment process. \
\pard\pardeftab720\sa240\partightenfactor0

\f0\b \cf0 \outl0\strokewidth0 \strokec2 Deep Q-Network (DQN)
\f1\b0  is a type of 
\f0\b reinforcement learning
\f1\b0  where a trading agent learns 
\f0\b how to make decisions
\f1\b0  like 
\f0\b buy, sell, or hold
\f1\b0  by interacting with historical market data.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls1\ilvl0\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The agent sees a 
\f0\b snapshot of the market
\f1\b0  (like prices and indicators).\
\ls1\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 It picks an 
\f0\b action
\f1\b0  (buy, sell, or hold).\
\ls1\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 It gets a 
\f0\b reward
\f1\b0 , usually profit or loss.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Over time, it 
\f0\b learns a strategy
\f1\b0  that maximizes total profit by updating a 
\f0\b neural network
\f1\b0  that estimates the best actions in each situation.\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \outl0\strokewidth0 \
\pard\pardeftab720\sa280\partightenfactor0

\f0\b\fs28 \cf0 \outl0\strokewidth0 \strokec2 Use Case: Algorithmic trading agents that learn to buy/sell/hold assets to maximize returns.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls2\ilvl0
\fs24 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 What DQN learns
\f1\b0 : When to enter or exit trades based on price history, indicators (e.g. EMA, MACD), and news sentiment.\
\ls2\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Environment
\f1\b0 : Market data (OHLCV, indicators).\
\ls2\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Action Space
\f1\b0 : Buy, Sell, Hold.\
\ls2\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Reward
\f1\b0 : Profit/loss or portfolio value change.\
\pard\pardeftab720\sa240\partightenfactor0

\f0\b \cf0 \strokec2 As an example
\f1\b0 , if the price is going up and momentum is strong, the agent might learn to buy. If the price is falling fast, it might learn to sell or hold to avoid losses. The goal is to automatically learn a trading strategy that makes profitable decisions without being explicitly programmed with trading rules.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\pard\pardeftab720\sa280\partightenfactor0

\f0\b\fs28 \cf0 \strokec2 Use Case: Portfolio Optimization \'97 allocates capital among multiple assets to maximize return and minimize risk.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls3\ilvl0
\fs24 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 What DQN learns
\f1\b0 : Rebalancing a portfolio weekly/monthly using DQN to learn optimal asset weights; learning to increase exposure to trending sectors or reduce allocation to volatile ones; managing ETF or index-based portfolios.\
\ls3\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Environment
\f1\b0 : Market data (OHLCV), portfolio data (weights, returns), investor\'92s risk profile or constraints.\
\ls3\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Action Space
\f1\b0 : Adjust position sizes or allocation percentages for each asset (i.e., portfolio loadings).\
\ls3\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Reward
\f1\b0 : Change in overall portfolio value, possibly adjusted for risk (e.g., Sharpe ratio or drawdown).\
\pard\pardeftab720\sa240\partightenfactor0

\f0\b \cf0 \strokec2 As an example
\f1\b0 , the agent may learn to increase weights in tech stocks during bull runs and shift to bonds or defensive assets during uncertainty. It adapts allocations to maintain optimal diversification and maximize long-term returns.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\pard\pardeftab720\sa280\partightenfactor0

\f0\b\fs28 \cf0 \strokec2 Use Case: Market Making / Order Execution \'97 optimizes order placement and execution strategies to reduce costs and improve trade efficiency.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls4\ilvl0
\fs24 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 What DQN learns
\f1\b0 : Optimize the placement of buy/sell orders to capture spread and reduce slippage. The DQN agent places limit orders at various depths in the order book to maximize profit from the bid-ask spread. It also learns how to split large orders over time to minimize market impact and adverse price movement.\
\ls4\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Environment
\f1\b0 : Real-time order book data, trade history, market depth, execution latency.\
\ls4\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Action Space
\f1\b0 : Place/cancel limit orders at different price levels; select order size; choose timing for execution.\
\ls4\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Reward
\f1\b0 : Execution quality \'97 measured by realized spread, price improvement, or cost savings (e.g., minimized slippage or market impact).\
\pard\pardeftab720\sa240\partightenfactor0

\f0\b \cf0 \strokec2 As an example
\f1\b0 , the agent may learn to post a small limit order just inside the spread when volatility is low, or wait before executing a large order if it predicts short-term price dips, reducing execution cost.\
\pard\pardeftab720\sa280\partightenfactor0
\cf0 \outl0\strokewidth0 \
\pard\pardeftab720\sa298\partightenfactor0

\f0\b\fs36 \cf0 Tools Used\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls5\ilvl0
\f2\b0\fs26 \cf0 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Python
\f1\fs24 \
\ls5\ilvl0
\f2\fs26 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
TensorFlow
\f1\fs24  or 
\f2\fs26 PyTorch
\f1\fs24  (here: 
\f0\b PyTorch
\f1\b0  for simplicity)\
\ls5\ilvl0
\f2\fs26 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
NumPy
\f1\fs24  and 
\f2\fs26 Pandas
\f1\fs24  for data\
\ls5\ilvl0
\f2\fs26 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Matplotlib
\f1\fs24  for visualization (optional)\
\pard\tx720\pardeftab720\sa240\partightenfactor0
\cf0 \
import pandas as pd\
data = pd.read_csv("AAPL.csv")  # Assume you have AAPL historical data\
prices = data['Close'].values\
import numpy as np\
\
class TradingEnv:\
    def __init__(self, prices, window_size=10):\
        self.prices = prices\
        self.window_size = window_size\
        self.reset()\
\
    def reset(self):\
        self.current_step = self.window_size\
        self.inventory = []\
        self.total_profit = 0\
        return self._get_state()\
\
    def _get_state(self):\
        return self.prices[self.current_step - self.window_size:self.current_step]\
\
    def step(self, action):\
        price = self.prices[self.current_step]\
        reward = 0\
\
        if action == 1:  # Buy\
            self.inventory.append(price)\
\
        elif action == 2 and len(self.inventory) > 0:  # Sell\
            bought_price = self.inventory.pop(0)\
            reward = price - bought_price\
            self.total_profit += reward\
\
        self.current_step += 1\
        done = self.current_step >= len(self.prices) - 1\
        next_state = self._get_state()\
        return next_state, reward, done\
\
import torch\
import torch.nn as nn\
import random\
\
class DQN(nn.Module):\
    def __init__(self, input_dim, output_dim):\
        super(DQN, self).__init__()\
        self.fc1 = nn.Linear(input_dim, 64)\
        self.fc2 = nn.Linear(64, 32)\
        self.fc3 = nn.Linear(32, output_dim)\
\
    def forward(self, x):\
        x = torch.relu(self.fc1(x))\
        x = torch.relu(self.fc2(x))\
        return self.fc3(x)\
\
from collections import deque\
\
env = TradingEnv(prices)\
model = DQN(env.window_size, 3)\
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\
loss_fn = nn.MSELoss()\
memory = deque(maxlen=1000)\
\
epsilon = 1.0\
gamma = 0.95\
batch_size = 32\
\
for episode in range(50):\
    state = torch.FloatTensor(env.reset())\
    total_reward = 0\
\
    while True:\
        if random.random() < epsilon:\
            action = random.randint(0, 2)\
        else:\
            with torch.no_grad():\
                q_values = model(state)\
                action = torch.argmax(q_values).item()\
\
        next_state, reward, done = env.step(action)\
        memory.append((state, action, reward, torch.FloatTensor(next_state), done))\
\
        if len(memory) > batch_size:\
            batch = random.sample(memory, batch_size)\
            states, actions, rewards, next_states, dones = zip(*batch)\
\
            states = torch.stack(states)\
            next_states = torch.stack(next_states)\
            actions = torch.tensor(actions)\
            rewards = torch.tensor(rewards, dtype=torch.float32)\
            dones = torch.tensor(dones, dtype=torch.bool)\
\
            q_values = model(states)\
            next_q_values = model(next_states)\
            targets = q_values.clone()\
\
            for i in range(batch_size):\
                targets[i, actions[i]] = rewards[i] + (0 if dones[i] else gamma * torch.max(next_q_values[i]))\
\
            loss = loss_fn(q_values, targets)\
            optimizer.zero_grad()\
            loss.backward()\
            optimizer.step()\
\
        state = torch.FloatTensor(next_state)\
        total_reward += reward\
\
        if done:\
            break\
\
    epsilon = max(0.01, epsilon * 0.995)\
    print(f"Episode \{episode\} - Total Profit: \{env.total_profit:.2f\}")\
\
\pard\pardeftab720\sa298\partightenfactor0

\f0\b\fs36 \cf0 Result\
\pard\pardeftab720\sa240\partightenfactor0

\f1\b0\fs24 \cf0 By the end of training, the agent learns:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls6\ilvl0\cf0 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
To buy low and sell high.\
\ls6\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
To avoid over-trading.\
\ls6\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
That holding during bad times is safer than wrong entries.\
}